{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, f1_score, roc_auc_score, r2_score, mean_squared_error, mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of      Question_ID                                      Comprehension  \\\n",
       "0            1.0  In Natural Language Processing, Feature Extrac...   \n",
       "1            1.0  In Natural Language Processing, Feature Extrac...   \n",
       "2            1.0  So we know that machines can only understand n...   \n",
       "3            1.0  So we know that machines can only understand n...   \n",
       "4            1.0  If we ask any NLP practitioner or data scienti...   \n",
       "..           ...                                                ...   \n",
       "691          4.0  n\\tSNOBOL (\"StriNg Oriented and symBOlic Langu...   \n",
       "692          4.0  n\\tSNOBOL (\"StriNg Oriented and symBOlic Langu...   \n",
       "693          4.0  Portability: \\nSo that the program can be move...   \n",
       "694          4.0  Portability: \\nSo that the program can be move...   \n",
       "695          4.0  Portability: \\nSo that the program can be move...   \n",
       "\n",
       "                                              Question  \\\n",
       "0             What is Feature Extraction from the text   \n",
       "1             What is Feature Extraction from the text   \n",
       "2      Why do we Need feature extraction for text data   \n",
       "3      Why do we Need feature extraction for text data   \n",
       "4    What makes feature extraction a difficukt task...   \n",
       "..                                                 ...   \n",
       "691  State the meaning of SNOBOL and give THREE exa...   \n",
       "692  State the meaning of SNOBOL and give THREE exa...   \n",
       "693  Distinguish between portability and interopera...   \n",
       "694  Distinguish between portability and interopera...   \n",
       "695  Distinguish between portability and interopera...   \n",
       "\n",
       "                                       Examiner_Answer  \\\n",
       "0    If we have textual data, that data we can not ...   \n",
       "1    If we have textual data, that data we can not ...   \n",
       "2    So we know that machines can only understand n...   \n",
       "3    So we know that machines can only understand n...   \n",
       "4    Featue extraction is difficult because it requ...   \n",
       "..                                                 ...   \n",
       "691  n\\tSNOBOL (\"StriNg Oriented and symBOlic Langu...   \n",
       "692  n\\tSNOBOL (\"StriNg Oriented and symBOlic Langu...   \n",
       "693  Portability: \\nSo that the program can be move...   \n",
       "694  Portability: \\nSo that the program can be move...   \n",
       "695  Portability: \\nSo that the program can be move...   \n",
       "\n",
       "                                        Student_Answer  Student_Score  \\\n",
       "0    The process of converting text data into numbe...              7   \n",
       "1                                      text processing              0   \n",
       "2    we need  feature extraction for text to conver...              9   \n",
       "3                                         numeric form              0   \n",
       "4                                        it takes time              0   \n",
       "..                                                 ...            ...   \n",
       "691  SNOBOL stands for \"StriNg Oriented and symBOli...              2   \n",
       "692                                             SNOBOL              0   \n",
       "693  Portability: \\nSo that the program can be move...              2   \n",
       "694  Interoperability concerns achieving functional...              2   \n",
       "695                                      Portability:               0   \n",
       "\n",
       "     Question_Score  \n",
       "0                10  \n",
       "1                10  \n",
       "2                10  \n",
       "3                10  \n",
       "4                10  \n",
       "..              ...  \n",
       "691               4  \n",
       "692               4  \n",
       "693               3  \n",
       "694               3  \n",
       "695               3  \n",
       "\n",
       "[696 rows x 7 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from CSV\n",
    "df = pd.read_csv('./Essay Grading Dataset/Essay Grading Dataset JET.csv', encoding='Latin-1')\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove non-alphanumeric characters and keep spaces\n",
    "    text = ''.join([char for char in text if char.isalnum() or char.isspace()])\n",
    "    print('text:', text[0])\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "#     tokens = nltk.word_tokenize(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    print('token:',tokens[0])\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Join the preprocessed tokens back into a single string\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    residual = np.abs(y_true - y_pred)\n",
    "    quadratic_loss = 0.5 * (residual ** 2)\n",
    "    linear_loss = delta * (residual - 0.5 * delta)\n",
    "    \n",
    "    loss = np.where(residual <= delta, quadratic_loss, linear_loss)\n",
    "    \n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      In Natural Language Processing, Feature Extrac...\n",
      "1      In Natural Language Processing, Feature Extrac...\n",
      "2      So we know that machines can only understand n...\n",
      "3      So we know that machines can only understand n...\n",
      "4      If we ask any NLP practitioner or data scienti...\n",
      "                             ...                        \n",
      "691    n\\tSNOBOL (\"StriNg Oriented and symBOlic Langu...\n",
      "692    n\\tSNOBOL (\"StriNg Oriented and symBOlic Langu...\n",
      "693    Portability: \\nSo that the program can be move...\n",
      "694    Portability: \\nSo that the program can be move...\n",
      "695    Portability: \\nSo that the program can be move...\n",
      "Length: 696, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Combine text features\n",
    "X_text = df['Comprehension'] + ' ' + df['Question'] + ' ' + df['Examiner_Answer'] + ' ' + df['Student_Answer']\n",
    "print(X_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Question_Score\n",
      "0                10\n",
      "1                10\n",
      "2                10\n",
      "3                10\n",
      "4                10\n",
      "..              ...\n",
      "691               4\n",
      "692               4\n",
      "693               3\n",
      "694               3\n",
      "695               3\n",
      "\n",
      "[696 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Numerical features\n",
    "X_numeric = df[['Question_Score']]\n",
    "print(X_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: i\n",
      "token: in\n",
      "text: i\n",
      "token: in\n",
      "text: s\n",
      "token: so\n",
      "text: s\n",
      "token: so\n",
      "text: i\n",
      "token: if\n",
      "text: i\n",
      "token: if\n",
      "text: a\n",
      "token: artificial\n",
      "text: a\n",
      "token: artificial\n",
      "text: a\n",
      "token: artificial\n",
      "text: a\n",
      "token: artificial\n",
      "text: a\n",
      "token: artificial\n",
      "text: a\n",
      "token: all\n",
      "text: a\n",
      "token: all\n",
      "text: a\n",
      "token: all\n",
      "text: a\n",
      "token: as\n",
      "text: a\n",
      "token: as\n",
      "text: a\n",
      "token: ai\n",
      "text: a\n",
      "token: ai\n",
      "text: a\n",
      "token: ai\n",
      "text: a\n",
      "token: artificial\n",
      "text: a\n",
      "token: arend\n",
      "text: a\n",
      "token: arend\n",
      "text: a\n",
      "token: arend\n",
      "text: a\n",
      "token: arend\n",
      "text: a\n",
      "token: arend\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: types\n",
      "text: s\n",
      "token: software\n",
      "text: i\n",
      "token: importance\n",
      "text: b\n",
      "token: benefits\n",
      "text: u\n",
      "token: unit\n",
      "text: i\n",
      "token: integration\n",
      "text: a\n",
      "token: a\n",
      "text: a\n",
      "token: a\n",
      "text: i\n",
      "token: in\n",
      "text: t\n",
      "token: the\n",
      "text: p\n",
      "token: pairwise\n",
      "text: a\n",
      "token: advantages\n",
      "text: s\n",
      "token: state\n",
      "text: s\n",
      "token: state\n",
      "text: a\n",
      "token: advantages\n",
      "text: a\n",
      "token: advantages\n",
      "text: f\n",
      "token: functional\n",
      "text: f\n",
      "token: functional\n",
      "text: p\n",
      "token: purpose\n",
      "text: p\n",
      "token: purpose\n",
      "text: t\n",
      "token: type\n",
      "text: p\n",
      "token: performance\n",
      "text: t\n",
      "token: types\n",
      "text: f\n",
      "token: features\n",
      "text: a\n",
      "token: a\n",
      "text: a\n",
      "token: a\n",
      "text: a\n",
      "token: a\n",
      "text: a\n",
      "token: a\n",
      "text: t\n",
      "token: the\n",
      "text: o\n",
      "token: objectives\n",
      "text: o\n",
      "token: objectives\n",
      "text: o\n",
      "token: objectives\n",
      "text: o\n",
      "token: objectives\n",
      "text: o\n",
      "token: objectives\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text:  \n",
      "token: the\n",
      "text:  \n",
      "token: the\n",
      "text:  \n",
      "token: the\n",
      "text: y\n",
      "token: yes\n",
      "text: y\n",
      "token: yes\n",
      "text: y\n",
      "token: yes\n",
      "text: t\n",
      "token: top\n",
      "text: s\n",
      "token: some\n",
      "text: s\n",
      "token: some\n",
      "text: s\n",
      "token: systems\n",
      "text: s\n",
      "token: systems\n",
      "text: k\n",
      "token: knowledge\n",
      "text: k\n",
      "token: knowledge\n",
      "text: k\n",
      "token: knowledge\n",
      "text: k\n",
      "token: knowledge\n",
      "text: k\n",
      "token: knowledge\n",
      "text: 1\n",
      "token: 1\n",
      "text: 1\n",
      "token: 1\n",
      "text: 1\n",
      "token: 1\n",
      "text: 1\n",
      "token: 1\n",
      "text: c\n",
      "token: classification\n",
      "text: c\n",
      "token: classification\n",
      "text: \t\n",
      "token: expertise\n",
      "text: \t\n",
      "token: expertise\n",
      "text: 1\n",
      "token: 1\n",
      "text: t\n",
      "token: types\n",
      "text: t\n",
      "token: types\n",
      "text: m\n",
      "token: model\n",
      "text: l\n",
      "token: learning\n",
      "text: l\n",
      "token: learning\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: s\n",
      "token: software\n",
      "text: s\n",
      "token: software\n",
      "text: s\n",
      "token: software\n",
      "text: s\n",
      "token: software\n",
      "text: c\n",
      "token: cost\n",
      "text: c\n",
      "token: cost\n",
      "text: c\n",
      "token: cost\n",
      "text: \t\n",
      "token: syntax\n",
      "text: \t\n",
      "token: syntax\n",
      "text: \t\n",
      "token: syntax\n",
      "text: \t\n",
      "token: syntax\n",
      "text: c\n",
      "token: characteristics\n",
      "text: c\n",
      "token: characteristics\n",
      "text: c\n",
      "token: characteristics\n",
      "text: c\n",
      "token: characteristics\n",
      "text: s\n",
      "token: sql\n",
      "text: s\n",
      "token: sql\n",
      "text: s\n",
      "token: sql\n",
      "text: s\n",
      "token: sql\n",
      "text: s\n",
      "token: sql\n",
      "text: r\n",
      "token: ruby\n",
      "text: r\n",
      "token: ruby\n",
      "text: r\n",
      "token: ruby\n",
      "text: r\n",
      "token: ruby\n",
      "text: \t\n",
      "token: advantages\n",
      "text: \t\n",
      "token: advantages\n",
      "text: \t\n",
      "token: advantages\n",
      "text: a\n",
      "token: algol\n",
      "text: a\n",
      "token: algol\n",
      "text: 1\n",
      "token: 1\n",
      "text: 1\n",
      "token: 1\n",
      "text: 1\n",
      "token: 1\n",
      "text: b\n",
      "token: backward\n",
      "text: b\n",
      "token: backward\n",
      "text: b\n",
      "token: backward\n",
      "text: m\n",
      "token: modular\n",
      "text: m\n",
      "token: modular\n",
      "text: a\n",
      "token: abstraction\n",
      "text: a\n",
      "token: abstraction\n",
      "text: f\n",
      "token: features\n",
      "text: f\n",
      "token: features\n",
      "text: f\n",
      "token: features\n",
      "text: f\n",
      "token: features\n",
      "text: f\n",
      "token: features\n",
      "text: t\n",
      "token: they\n",
      "text: t\n",
      "token: they\n",
      "text: t\n",
      "token: they\n",
      "text: f\n",
      "token: features\n",
      "text: f\n",
      "token: features\n",
      "text: f\n",
      "token: features\n",
      "text: s\n",
      "token: static\n",
      "text: s\n",
      "token: static\n",
      "text: d\n",
      "token: design\n",
      "text: d\n",
      "token: design\n",
      "text: d\n",
      "token: design\n",
      "text: d\n",
      "token: design\n",
      "text: s\n",
      "token: snobol\n",
      "text: n\n",
      "token: n\n",
      "text: n\n",
      "token: n\n",
      "text: p\n",
      "token: portability\n",
      "text: p\n",
      "token: portability\n",
      "text: p\n",
      "token: portability\n",
      "text: i\n",
      "token: in\n",
      "text: i\n",
      "token: in\n",
      "text: s\n",
      "token: so\n",
      "text: s\n",
      "token: so\n",
      "text: i\n",
      "token: if\n",
      "text: i\n",
      "token: if\n",
      "text: a\n",
      "token: artificial\n",
      "text: a\n",
      "token: artificial\n",
      "text: a\n",
      "token: artificial\n",
      "text: a\n",
      "token: artificial\n",
      "text: a\n",
      "token: artificial\n",
      "text: a\n",
      "token: all\n",
      "text: a\n",
      "token: all\n",
      "text: a\n",
      "token: all\n",
      "text: a\n",
      "token: as\n",
      "text: a\n",
      "token: as\n",
      "text: a\n",
      "token: ai\n",
      "text: a\n",
      "token: ai\n",
      "text: a\n",
      "token: ai\n",
      "text: a\n",
      "token: artificial\n",
      "text: a\n",
      "token: arend\n",
      "text: a\n",
      "token: arend\n",
      "text: a\n",
      "token: arend\n",
      "text: a\n",
      "token: arend\n",
      "text: a\n",
      "token: arend\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: types\n",
      "text: s\n",
      "token: software\n",
      "text: i\n",
      "token: importance\n",
      "text: b\n",
      "token: benefits\n",
      "text: u\n",
      "token: unit\n",
      "text: i\n",
      "token: integration\n",
      "text: a\n",
      "token: a\n",
      "text: a\n",
      "token: a\n",
      "text: i\n",
      "token: in\n",
      "text: t\n",
      "token: the\n",
      "text: p\n",
      "token: pairwise\n",
      "text: a\n",
      "token: advantages\n",
      "text: s\n",
      "token: state\n",
      "text: s\n",
      "token: state\n",
      "text: a\n",
      "token: advantages\n",
      "text: a\n",
      "token: advantages\n",
      "text: f\n",
      "token: functional\n",
      "text: f\n",
      "token: functional\n",
      "text: p\n",
      "token: purpose\n",
      "text: p\n",
      "token: purpose\n",
      "text: t\n",
      "token: type\n",
      "text: p\n",
      "token: performance\n",
      "text: t\n",
      "token: types\n",
      "text: f\n",
      "token: features\n",
      "text: a\n",
      "token: a\n",
      "text: a\n",
      "token: a\n",
      "text: a\n",
      "token: a\n",
      "text: a\n",
      "token: a\n",
      "text: t\n",
      "token: the\n",
      "text: o\n",
      "token: objectives\n",
      "text: o\n",
      "token: objectives\n",
      "text: o\n",
      "token: objectives\n",
      "text: o\n",
      "token: objectives\n",
      "text: o\n",
      "token: objectives\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text:  \n",
      "token: the\n",
      "text:  \n",
      "token: the\n",
      "text:  \n",
      "token: the\n",
      "text: y\n",
      "token: yes\n",
      "text: y\n",
      "token: yes\n",
      "text: y\n",
      "token: yes\n",
      "text: t\n",
      "token: top\n",
      "text: s\n",
      "token: some\n",
      "text: s\n",
      "token: some\n",
      "text: s\n",
      "token: systems\n",
      "text: s\n",
      "token: systems\n",
      "text: k\n",
      "token: knowledge\n",
      "text: k\n",
      "token: knowledge\n",
      "text: k\n",
      "token: knowledge\n",
      "text: k\n",
      "token: knowledge\n",
      "text: k\n",
      "token: knowledge\n",
      "text: 1\n",
      "token: 1\n",
      "text: 1\n",
      "token: 1\n",
      "text: 1\n",
      "token: 1\n",
      "text: 1\n",
      "token: 1\n",
      "text: c\n",
      "token: classification\n",
      "text: c\n",
      "token: classification\n",
      "text: \t\n",
      "token: expertise\n",
      "text: \t\n",
      "token: expertise\n",
      "text: 1\n",
      "token: 1\n",
      "text: t\n",
      "token: types\n",
      "text: t\n",
      "token: types\n",
      "text: m\n",
      "token: model\n",
      "text: l\n",
      "token: learning\n",
      "text: l\n",
      "token: learning\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: s\n",
      "token: software\n",
      "text: s\n",
      "token: software\n",
      "text: s\n",
      "token: software\n",
      "text: s\n",
      "token: software\n",
      "text: c\n",
      "token: cost\n",
      "text: c\n",
      "token: cost\n",
      "text: c\n",
      "token: cost\n",
      "text: \t\n",
      "token: syntax\n",
      "text: \t\n",
      "token: syntax\n",
      "text: \t\n",
      "token: syntax\n",
      "text: \t\n",
      "token: syntax\n",
      "text: c\n",
      "token: characteristics\n",
      "text: c\n",
      "token: characteristics\n",
      "text: c\n",
      "token: characteristics\n",
      "text: c\n",
      "token: characteristics\n",
      "text: s\n",
      "token: sql\n",
      "text: s\n",
      "token: sql\n",
      "text: s\n",
      "token: sql\n",
      "text: s\n",
      "token: sql\n",
      "text: s\n",
      "token: sql\n",
      "text: r\n",
      "token: ruby\n",
      "text: r\n",
      "token: ruby\n",
      "text: r\n",
      "token: ruby\n",
      "text: r\n",
      "token: ruby\n",
      "text: \t\n",
      "token: advantages\n",
      "text: \t\n",
      "token: advantages\n",
      "text: \t\n",
      "token: advantages\n",
      "text: a\n",
      "token: algol\n",
      "text: a\n",
      "token: algol\n",
      "text: 1\n",
      "token: 1\n",
      "text: 1\n",
      "token: 1\n",
      "text: 1\n",
      "token: 1\n",
      "text: b\n",
      "token: backward\n",
      "text: b\n",
      "token: backward\n",
      "text: b\n",
      "token: backward\n",
      "text: m\n",
      "token: modular\n",
      "text: m\n",
      "token: modular\n",
      "text: a\n",
      "token: abstraction\n",
      "text: a\n",
      "token: abstraction\n",
      "text: f\n",
      "token: features\n",
      "text: f\n",
      "token: features\n",
      "text: f\n",
      "token: features\n",
      "text: f\n",
      "token: features\n",
      "text: f\n",
      "token: features\n",
      "text: t\n",
      "token: they\n",
      "text: t\n",
      "token: they\n",
      "text: t\n",
      "token: they\n",
      "text: f\n",
      "token: features\n",
      "text: f\n",
      "token: features\n",
      "text: f\n",
      "token: features\n",
      "text: s\n",
      "token: static\n",
      "text: s\n",
      "token: static\n",
      "text: d\n",
      "token: design\n",
      "text: d\n",
      "token: design\n",
      "text: d\n",
      "token: design\n",
      "text: d\n",
      "token: design\n",
      "text: s\n",
      "token: snobol\n",
      "text: n\n",
      "token: n\n",
      "text: n\n",
      "token: n\n",
      "text: p\n",
      "token: portability\n",
      "text: p\n",
      "token: portability\n",
      "text: p\n",
      "token: portability\n",
      "text: i\n",
      "token: in\n",
      "text: i\n",
      "token: in\n",
      "text: s\n",
      "token: so\n",
      "text: s\n",
      "token: so\n",
      "text: i\n",
      "token: if\n",
      "text: i\n",
      "token: if\n",
      "text: a\n",
      "token: artificial\n",
      "text: a\n",
      "token: artificial\n",
      "text: a\n",
      "token: artificial\n",
      "text: a\n",
      "token: artificial\n",
      "text: a\n",
      "token: artificial\n",
      "text: a\n",
      "token: all\n",
      "text: a\n",
      "token: all\n",
      "text: a\n",
      "token: all\n",
      "text: a\n",
      "token: as\n",
      "text: a\n",
      "token: as\n",
      "text: a\n",
      "token: ai\n",
      "text: a\n",
      "token: ai\n",
      "text: a\n",
      "token: ai\n",
      "text: a\n",
      "token: artificial\n",
      "text: a\n",
      "token: arend\n",
      "text: a\n",
      "token: arend\n",
      "text: a\n",
      "token: arend\n",
      "text: a\n",
      "token: arend\n",
      "text: a\n",
      "token: arend\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: types\n",
      "text: s\n",
      "token: software\n",
      "text: i\n",
      "token: importance\n",
      "text: b\n",
      "token: benefits\n",
      "text: u\n",
      "token: unit\n",
      "text: i\n",
      "token: integration\n",
      "text: a\n",
      "token: a\n",
      "text: a\n",
      "token: a\n",
      "text: i\n",
      "token: in\n",
      "text: t\n",
      "token: the\n",
      "text: p\n",
      "token: pairwise\n",
      "text: a\n",
      "token: advantages\n",
      "text: s\n",
      "token: state\n",
      "text: s\n",
      "token: state\n",
      "text: a\n",
      "token: advantages\n",
      "text: a\n",
      "token: advantages\n",
      "text: f\n",
      "token: functional\n",
      "text: f\n",
      "token: functional\n",
      "text: p\n",
      "token: purpose\n",
      "text: p\n",
      "token: purpose\n",
      "text: t\n",
      "token: type\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: p\n",
      "token: performance\n",
      "text: t\n",
      "token: types\n",
      "text: f\n",
      "token: features\n",
      "text: a\n",
      "token: a\n",
      "text: a\n",
      "token: a\n",
      "text: a\n",
      "token: a\n",
      "text: a\n",
      "token: a\n",
      "text: t\n",
      "token: the\n",
      "text: o\n",
      "token: objectives\n",
      "text: o\n",
      "token: objectives\n",
      "text: o\n",
      "token: objectives\n",
      "text: o\n",
      "token: objectives\n",
      "text: o\n",
      "token: objectives\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text:  \n",
      "token: the\n",
      "text:  \n",
      "token: the\n",
      "text:  \n",
      "token: the\n",
      "text: y\n",
      "token: yes\n",
      "text: y\n",
      "token: yes\n",
      "text: y\n",
      "token: yes\n",
      "text: t\n",
      "token: top\n",
      "text: s\n",
      "token: some\n",
      "text: s\n",
      "token: some\n",
      "text: s\n",
      "token: systems\n",
      "text: s\n",
      "token: systems\n",
      "text: k\n",
      "token: knowledge\n",
      "text: k\n",
      "token: knowledge\n",
      "text: k\n",
      "token: knowledge\n",
      "text: k\n",
      "token: knowledge\n",
      "text: k\n",
      "token: knowledge\n",
      "text: 1\n",
      "token: 1\n",
      "text: 1\n",
      "token: 1\n",
      "text: 1\n",
      "token: 1\n",
      "text: 1\n",
      "token: 1\n",
      "text: c\n",
      "token: classification\n",
      "text: c\n",
      "token: classification\n",
      "text: \t\n",
      "token: expertise\n",
      "text: \t\n",
      "token: expertise\n",
      "text: 1\n",
      "token: 1\n",
      "text: t\n",
      "token: types\n",
      "text: t\n",
      "token: types\n",
      "text: m\n",
      "token: model\n",
      "text: l\n",
      "token: learning\n",
      "text: l\n",
      "token: learning\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: s\n",
      "token: software\n",
      "text: s\n",
      "token: software\n",
      "text: s\n",
      "token: software\n",
      "text: s\n",
      "token: software\n",
      "text: c\n",
      "token: cost\n",
      "text: c\n",
      "token: cost\n",
      "text: c\n",
      "token: cost\n",
      "text: \t\n",
      "token: syntax\n",
      "text: \t\n",
      "token: syntax\n",
      "text: \t\n",
      "token: syntax\n",
      "text: \t\n",
      "token: syntax\n",
      "text: c\n",
      "token: characteristics\n",
      "text: c\n",
      "token: characteristics\n",
      "text: c\n",
      "token: characteristics\n",
      "text: c\n",
      "token: characteristics\n",
      "text: s\n",
      "token: sql\n",
      "text: s\n",
      "token: sql\n",
      "text: s\n",
      "token: sql\n",
      "text: s\n",
      "token: sql\n",
      "text: s\n",
      "token: sql\n",
      "text: r\n",
      "token: ruby\n",
      "text: r\n",
      "token: ruby\n",
      "text: r\n",
      "token: ruby\n",
      "text: r\n",
      "token: ruby\n",
      "text: \t\n",
      "token: advantages\n",
      "text: \t\n",
      "token: advantages\n",
      "text: \t\n",
      "token: advantages\n",
      "text: a\n",
      "token: algol\n",
      "text: a\n",
      "token: algol\n",
      "text: 1\n",
      "token: 1\n",
      "text: 1\n",
      "token: 1\n",
      "text: 1\n",
      "token: 1\n",
      "text: b\n",
      "token: backward\n",
      "text: b\n",
      "token: backward\n",
      "text: b\n",
      "token: backward\n",
      "text: m\n",
      "token: modular\n",
      "text: m\n",
      "token: modular\n",
      "text: a\n",
      "token: abstraction\n",
      "text: a\n",
      "token: abstraction\n",
      "text: f\n",
      "token: features\n",
      "text: f\n",
      "token: features\n",
      "text: f\n",
      "token: features\n",
      "text: f\n",
      "token: features\n",
      "text: f\n",
      "token: features\n",
      "text: t\n",
      "token: they\n",
      "text: t\n",
      "token: they\n",
      "text: t\n",
      "token: they\n",
      "text: f\n",
      "token: features\n",
      "text: f\n",
      "token: features\n",
      "text: f\n",
      "token: features\n",
      "text: s\n",
      "token: static\n",
      "text: s\n",
      "token: static\n",
      "text: d\n",
      "token: design\n",
      "text: d\n",
      "token: design\n",
      "text: d\n",
      "token: design\n",
      "text: d\n",
      "token: design\n",
      "text: s\n",
      "token: snobol\n",
      "text: n\n",
      "token: n\n",
      "text: n\n",
      "token: n\n",
      "text: p\n",
      "token: portability\n",
      "text: p\n",
      "token: portability\n",
      "text: p\n",
      "token: portability\n",
      "text: i\n",
      "token: in\n",
      "text: i\n",
      "token: in\n",
      "text: s\n",
      "token: so\n",
      "text: s\n",
      "token: so\n",
      "text: i\n",
      "token: if\n",
      "text: i\n",
      "token: if\n",
      "text: a\n",
      "token: artificial\n",
      "text: a\n",
      "token: artificial\n",
      "text: a\n",
      "token: artificial\n",
      "text: a\n",
      "token: artificial\n",
      "text: a\n",
      "token: artificial\n",
      "text: a\n",
      "token: all\n",
      "text: a\n",
      "token: all\n",
      "text: a\n",
      "token: all\n",
      "text: a\n",
      "token: as\n",
      "text: a\n",
      "token: as\n",
      "text: a\n",
      "token: ai\n",
      "text: a\n",
      "token: ai\n",
      "text: a\n",
      "token: ai\n",
      "text: a\n",
      "token: artificial\n",
      "text: a\n",
      "token: arend\n",
      "text: a\n",
      "token: arend\n",
      "text: a\n",
      "token: arend\n",
      "text: a\n",
      "token: arend\n",
      "text: a\n",
      "token: arend\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: types\n",
      "text: s\n",
      "token: software\n",
      "text: i\n",
      "token: importance\n",
      "text: b\n",
      "token: benefits\n",
      "text: u\n",
      "token: unit\n",
      "text: i\n",
      "token: integration\n",
      "text: a\n",
      "token: a\n",
      "text: a\n",
      "token: a\n",
      "text: i\n",
      "token: in\n",
      "text: t\n",
      "token: the\n",
      "text: p\n",
      "token: pairwise\n",
      "text: a\n",
      "token: advantages\n",
      "text: s\n",
      "token: state\n",
      "text: s\n",
      "token: state\n",
      "text: a\n",
      "token: advantages\n",
      "text: a\n",
      "token: advantages\n",
      "text: f\n",
      "token: functional\n",
      "text: f\n",
      "token: functional\n",
      "text: p\n",
      "token: purpose\n",
      "text: p\n",
      "token: purpose\n",
      "text: t\n",
      "token: type\n",
      "text: p\n",
      "token: performance\n",
      "text: t\n",
      "token: types\n",
      "text: f\n",
      "token: features\n",
      "text: a\n",
      "token: a\n",
      "text: a\n",
      "token: a\n",
      "text: a\n",
      "token: a\n",
      "text: a\n",
      "token: a\n",
      "text: t\n",
      "token: the\n",
      "text: o\n",
      "token: objectives\n",
      "text: o\n",
      "token: objectives\n",
      "text: o\n",
      "token: objectives\n",
      "text: o\n",
      "token: objectives\n",
      "text: o\n",
      "token: objectives\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text: d\n",
      "token: data\n",
      "text:  \n",
      "token: the\n",
      "text:  \n",
      "token: the\n",
      "text:  \n",
      "token: the\n",
      "text: y\n",
      "token: yes\n",
      "text: y\n",
      "token: yes\n",
      "text: y\n",
      "token: yes\n",
      "text: t\n",
      "token: top\n",
      "text: s\n",
      "token: some\n",
      "text: s\n",
      "token: some\n",
      "text: s\n",
      "token: systems\n",
      "text: s\n",
      "token: systems\n",
      "text: k\n",
      "token: knowledge\n",
      "text: k\n",
      "token: knowledge\n",
      "text: k\n",
      "token: knowledge\n",
      "text: k\n",
      "token: knowledge\n",
      "text: k\n",
      "token: knowledge\n",
      "text: 1\n",
      "token: 1\n",
      "text: 1\n",
      "token: 1\n",
      "text: 1\n",
      "token: 1\n",
      "text: 1\n",
      "token: 1\n",
      "text: c\n",
      "token: classification\n",
      "text: c\n",
      "token: classification\n",
      "text: \t\n",
      "token: expertise\n",
      "text: \t\n",
      "token: expertise\n",
      "text: 1\n",
      "token: 1\n",
      "text: t\n",
      "token: types\n",
      "text: t\n",
      "token: types\n",
      "text: m\n",
      "token: model\n",
      "text: l\n",
      "token: learning\n",
      "text: l\n",
      "token: learning\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: t\n",
      "token: the\n",
      "text: s\n",
      "token: software\n",
      "text: s\n",
      "token: software\n",
      "text: s\n",
      "token: software\n",
      "text: s\n",
      "token: software\n",
      "text: c\n",
      "token: cost\n",
      "text: c\n",
      "token: cost\n",
      "text: c\n",
      "token: cost\n",
      "text: \t\n",
      "token: syntax\n",
      "text: \t\n",
      "token: syntax\n",
      "text: \t\n",
      "token: syntax\n",
      "text: \t\n",
      "token: syntax\n",
      "text: c\n",
      "token: characteristics\n",
      "text: c\n",
      "token: characteristics\n",
      "text: c\n",
      "token: characteristics\n",
      "text: c\n",
      "token: characteristics\n",
      "text: s\n",
      "token: sql\n",
      "text: s\n",
      "token: sql\n",
      "text: s\n",
      "token: sql\n",
      "text: s\n",
      "token: sql\n",
      "text: s\n",
      "token: sql\n",
      "text: r\n",
      "token: ruby\n",
      "text: r\n",
      "token: ruby\n",
      "text: r\n",
      "token: ruby\n",
      "text: r\n",
      "token: ruby\n",
      "text: \t\n",
      "token: advantages\n",
      "text: \t\n",
      "token: advantages\n",
      "text: \t\n",
      "token: advantages\n",
      "text: a\n",
      "token: algol\n",
      "text: a\n",
      "token: algol\n",
      "text: 1\n",
      "token: 1\n",
      "text: 1\n",
      "token: 1\n",
      "text: 1\n",
      "token: 1\n",
      "text: b\n",
      "token: backward\n",
      "text: b\n",
      "token: backward\n",
      "text: b\n",
      "token: backward\n",
      "text: m\n",
      "token: modular\n",
      "text: m\n",
      "token: modular\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing\n",
    "X_text_preprocessed = X_text.apply(preprocess_text)\n",
    "print(X_text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_text_vectorized = tfidf_vectorizer.fit_transform(X_text_preprocessed)\n",
    "print(X_text_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine numerical and text features\n",
    "X_combined_vectorized = pd.concat([X_numeric.reset_index(drop=True), pd.DataFrame(X_text_vectorized.toarray())], axis=1)\n",
    "print(X_combined_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variable\n",
    "y = df['Student_Score']\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined_vectorized, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-squared (R²)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'R-squared (R²): {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'Mean Absolute Error (MAE): {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build and train the model\n",
    "logistic_model = LogisticRegression(random_state=42)\n",
    "logistic_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_log = logistic_model.predict(X_test)\n",
    "print(y_pred_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred_log)\n",
    "r2 = r2_score(y_test, y_pred_log)\n",
    "mae = mean_absolute_error(y_test, y_pred_log)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared (R²): {r2}')\n",
    "print(f'Mean Absolute Error (MAE): {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the model\n",
    "xgb_model = xgb.XGBRegressor(objective ='reg:squarederror', random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "print(y_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "print(f'XGBoost Mean Squared Error: {mse_xgb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-squared (R²)\n",
    "r2 = r2_score(y_test, y_pred_xgb)\n",
    "print(f'R-squared (R²): {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = mean_squared_error(y_test, y_pred_xgb, squared=False)\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred_xgb)\n",
    "print(f'Mean Absolute Error (MAE): {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regressor (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the model\n",
    "svr_model = SVR()\n",
    "svr_model.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_svr = svr_model.predict(X_test_scaled)\n",
    "print(y_pred_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "print(f'SVR Mean Squared Error: {mse_svr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-squared (R²)\n",
    "r2 = r2_score(y_test, y_pred_svr)\n",
    "print(f'R-squared (R²): {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = mean_squared_error(y_test, y_pred_svr, squared=False)\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred_svr)\n",
    "print(f'Mean Absolute Error (MAE): {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "print(y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to display actual and predicted values\n",
    "results_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_rf})\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "print(f'Random Forest Mean Squared Error: {mse_rf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-squared (R²)\n",
    "r2 = r2_score(y_test, y_pred_rf)\n",
    "print(f'R-squared (R²): {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = mean_squared_error(y_test, y_pred_rf, squared=False)\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred_rf)\n",
    "print(f'Mean Absolute Error (MAE): {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 1.0\n",
    "loss = huber_loss(y_test, y_pred_rf, delta)\n",
    "\n",
    "print(f\"Huber Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train KNN Regressor\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=5)  # You can adjust the number of neighbors (k) as needed\n",
    "knn_regressor.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_knn = knn_regressor.predict(X_test)\n",
    "print(y_pred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to display actual and predicted values\n",
    "results_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_knn})\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred_knn)\n",
    "r2 = r2_score(y_test, y_pred_knn)\n",
    "mae = mean_absolute_error(y_test, y_pred_knn)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared (R²): {r2}')\n",
    "print(f'Mean Absolute Error (MAE): {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussain Naives Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the Gaussian Naive Bayes model\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "print(y_pred_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred_nb)\n",
    "r2 = r2_score(y_test, y_pred_nb)\n",
    "mae = mean_absolute_error(y_test, y_pred_nb)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared (R²): {r2}')\n",
    "print(f'Mean Absolute Error (MAE): {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the Decision Tree Regressor model\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "print(y_pred_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to display actual and predicted values\n",
    "results_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_dt})\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred_dt)\n",
    "r2 = r2_score(y_test, y_pred_dt)\n",
    "mae = mean_absolute_error(y_test, y_pred_dt)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared (R²): {r2}')\n",
    "print(f'Mean Absolute Error (MAE): {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 1.0\n",
    "loss = huber_loss(y_test, y_pred_dt, delta)\n",
    "\n",
    "print(f\"Huber Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Build and train the Ridge Regression model\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_ridge = ridge_model.predict(X_test_scaled)\n",
    "print(y_pred_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to display actual and predicted values\n",
    "results_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_knn})\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred_ridge)\n",
    "r2 = r2_score(y_test, y_pred_ridge)\n",
    "mae = mean_absolute_error(y_test, y_pred_ridge)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared (R²): {r2}')\n",
    "print(f'Mean Absolute Error (MAE): {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 1.0\n",
    "loss = huber_loss(y_test, y_pred_ridge, delta)\n",
    "\n",
    "print(f\"Huber Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Least Squares Regression (PLSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the PLSR model\n",
    "plsr_model = PLSRegression(n_components=2)\n",
    "plsr_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_plsr = plsr_model.predict(X_test_scaled)\n",
    "print(y_pred_plsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred_plsr)\n",
    "r2 = r2_score(y_test, y_pred_plsr)\n",
    "mae = mean_absolute_error(y_test, y_pred_plsr)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared (R²): {r2}')\n",
    "print(f'Mean Absolute Error (MAE): {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares Regression (OLSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant term to the features matrix\n",
    "X = sm.add_constant(X_combined_vectorized)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the OLS model\n",
    "ols_model = sm.OLS(y, X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted values\n",
    "y_pred_ols = ols_model.predict(X)\n",
    "print(y_pred_ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare actual vs. predicted values\n",
    "result_df = pd.DataFrame({'Actual': y, 'Predicted': y_pred_ols})\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the summary of the model\n",
    "print(ols_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 1.0\n",
    "loss = huber_loss(y, y_pred_ols, delta)\n",
    "\n",
    "print(f\"Huber Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dumped DecisionTree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt_model\n",
    "# Save the model to a file using joblib\n",
    "joblib.dump(dt_model, 'dt_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('dt_model.pkl', 'wb') as f:\n",
    "    pickle.dump(dt_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt_model\n",
    "# Save the model to a file using joblib\n",
    "joblib.dump(dt_model, 'dt_model.new')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
